{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eebf7324",
      "metadata": {
        "id": "eebf7324"
      },
      "source": [
        "## Setup and utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7e1ded",
      "metadata": {
        "id": "8c7e1ded"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request as request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "\n",
        "def degree_matrix(A):\n",
        "    d = A.sum(axis=1)\n",
        "    return np.diag(d)\n",
        "import numpy as np\n",
        "\n",
        "def laplacian(A, normalized=False):\n",
        "    D = np.diag(A.sum(axis=1))\n",
        "\n",
        "    if normalized:\n",
        "        D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))\n",
        "        I = np.eye(A.shape[0])\n",
        "        return I - np.dot(np.dot(D_inv_sqrt, A), D_inv_sqrt)\n",
        "    else:\n",
        "        return D - A\n",
        "\n",
        "\n",
        "def scaled_laplacian(L):\n",
        "\n",
        "    lambda_max = np.linalg.eigvalsh(L).max()\n",
        "    I = np.eye(L.shape[0])\n",
        "    return (2 / lambda_max) * L - I\n",
        "\n",
        "def get_largest_eigenvalue(mat):\n",
        "    return float(np.linalg.eigvalsh(mat).max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d904d3a5",
      "metadata": {
        "id": "d904d3a5"
      },
      "source": [
        "## 1. Load the Cora dataset\n",
        "The Cora dataset represents academic publications and citations, consisting of 2708 nodes (papers) and 5429 edges (citations). Each node has a binary word vector of size 1433 where each value represents the absence/presence of a specific word. The publications are assigned to one of seven classes, representing the field of the publication. Here use the dataset to predict the field of papers.\n",
        "\n",
        "![Visualization of the Cora dataset](https://graphsandnetworks.com/wp-content/uploads/2019/09/CoraBalloons.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37e02cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a37e02cb",
        "outputId": "6c98f92b-d42b-4fd5-8e9b-63165fba2f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Cora dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3533348425.py:16: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=DATA_DIR)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted to ./\n",
            "Nodes: 2708 Features: 1433 Classes: 7\n",
            "Adjacency shape: (2708, 2708)\n"
          ]
        }
      ],
      "source": [
        "URL = \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\"\n",
        "DATA_DIR = \"./\"\n",
        "TGZ_PATH = os.path.join(DATA_DIR, \"cora.tgz\")\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "### Download\n",
        "if not os.path.exists(TGZ_PATH):\n",
        "    print(\"Downloading Cora dataset...\")\n",
        "    request.urlretrieve(URL, TGZ_PATH)\n",
        "else:\n",
        "    print(\"File already exists:\", TGZ_PATH)\n",
        "\n",
        "### Extract\n",
        "with tarfile.open(TGZ_PATH, \"r:gz\") as tar:\n",
        "    tar.extractall(path=DATA_DIR)\n",
        "print(\"Extracted to\", DATA_DIR)\n",
        "\n",
        "### Load cora.content, which contains nodes, features and labels\n",
        "content_path = os.path.join(DATA_DIR, \"cora\", \"cora.content\")\n",
        "ids = []\n",
        "features = []\n",
        "labels_raw = []\n",
        "with open(content_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) < 3:\n",
        "            continue\n",
        "        ids.append(parts[0])\n",
        "        features.append([float(x) for x in parts[1:-1]])\n",
        "        labels_raw.append(parts[-1])\n",
        "\n",
        "ids = np.array(ids)\n",
        "X = np.array(features, dtype=float) # matrix of dimensions (samples,features)\n",
        "\n",
        "### Label encoding\n",
        "classes = sorted(set(labels_raw))\n",
        "label_map = {c: i for i, c in enumerate(classes)}\n",
        "y = np.array([label_map[c] for c in labels_raw], dtype=int)\n",
        "\n",
        "### Load cora.cites, which contains the edges, and store it in an adjacency matrix\n",
        "n = len(ids)\n",
        "# For each id (node identifiers given by the dataset), get the index (idx)\n",
        "id_to_idx = {pid: i for i, pid in enumerate(ids)}\n",
        "A = np.zeros((n, n), dtype=float)\n",
        "\n",
        "cites_path = os.path.join(DATA_DIR, \"cora\", \"cora.cites\")\n",
        "with open(cites_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        src, dst = parts\n",
        "        if src in id_to_idx and dst in id_to_idx:\n",
        "            i, j = id_to_idx[src], id_to_idx[dst]\n",
        "            A[i, j] = 1.0\n",
        "            A[j, i] = 1.0  # undirected\n",
        "\n",
        "print(\"Nodes:\", n, \"Features:\", X.shape[1], \"Classes:\", len(classes))\n",
        "print(\"Adjacency shape:\", A.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067467f3",
      "metadata": {
        "id": "067467f3"
      },
      "source": [
        "## 1. Laplacian and scaled Laplacian\n",
        "The Laplacian $L=D-A$ contains both the adjacency structure as well as the degree of nodes, essentially describing the whole graph. The normalization $L = I - D^{-1/2} A D^{-1/2}$ has a number of favorable properties mostly relevant for spectral graph theory; for the case of graph neural networks, the normalization can mostly be seen as reducing the impact of high-degree nodes relative to other nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e533123",
      "metadata": {
        "id": "9e533123"
      },
      "outputs": [],
      "source": [
        "L = laplacian(A, normalized=True)\n",
        "L_tilde = scaled_laplacian(L)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9356c794",
      "metadata": {
        "id": "9356c794"
      },
      "source": [
        "## 2. Chebyshev polynomials on graphs\n",
        "Chebyshev polynomials are used for approximations where uniform accuracy over the whole domain [-1,1] is relevant (compared to Taylor series, which are locally accurate).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6c45d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff6c45d7",
        "outputId": "f435a14a-0b7e-4835-8001-efe7922e8178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K=0 [(2708, 1433)]\n",
            "K=1 [(2708, 1433), (2708, 1433)]\n",
            "K=2 [(2708, 1433), (2708, 1433), (2708, 1433)]\n",
            "K=3 [(2708, 1433), (2708, 1433), (2708, 1433), (2708, 1433)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def chebyshev_polynomials(L_tilde, X, K):\n",
        "    Ts = [X]  # T0 = X\n",
        "\n",
        "    if K >= 1:\n",
        "        T1 = np.dot(L_tilde, X)\n",
        "        Ts.append(T1)\n",
        "\n",
        "        for _ in range(2, K + 1):\n",
        "            T2 = 2 * np.dot(L_tilde, Ts[-1]) - Ts[-2]\n",
        "            Ts.append(T2)\n",
        "\n",
        "    return Ts\n",
        "\n",
        "\n",
        "for K in [0,1,2,3]:\n",
        "    Ts = chebyshev_polynomials(L_tilde, X, K)\n",
        "    print(f\"K={K}\", [t.shape for t in Ts]) # Should be the same shape for every K, but +1 entry.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6e0816b",
      "metadata": {
        "id": "c6e0816b"
      },
      "source": [
        "## 3. ChebNet layer\n",
        "The idea of ChebNet is that since convolution in the graph domain corresponds to the element-wise product in the spectral domain (Eigenvalue decomposition; $h*g = U\\hat GU^Th$), the spectral filters $\\hat g_v$ for nodes $v$ that make up $\\hat G$ can be approximated. These approximations are parameterized with learned coefficients $\\theta$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0897d06",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0897d06",
        "outputId": "f29e69f8-37d1-49a2-c65f-7f02ada0071d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z_demo shape: (2708, 10)\n"
          ]
        }
      ],
      "source": [
        "def chebnet_forward_pass(TX, thetas):\n",
        "\n",
        "    Z = np.zeros((TX[0].shape[0], thetas[0].shape[1]))\n",
        "\n",
        "    for T_k, theta_k in zip(TX, thetas):\n",
        "        Z += np.dot(T_k, theta_k)\n",
        "\n",
        "    return Z\n",
        "\n",
        "### For demo: Map the input feature dimension to the output feature dimension of 10\n",
        "F_in, F_out = X.shape[1], 10\n",
        "K = 2\n",
        "thetas_init = [np.random.randn(F_in, F_out)*0.1 for _ in range(K+1)]\n",
        "\n",
        "### Compute polynomials once, then perform a forward pass\n",
        "TX = chebyshev_polynomials(L_tilde, X, K)\n",
        "Z_demo = chebnet_forward_pass(TX, thetas_init)\n",
        "print(\"Z_demo shape:\", Z_demo.shape) #\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d770b20",
      "metadata": {
        "id": "9d770b20"
      },
      "source": [
        "## 4. Semi-supervised split\n",
        "Use a small labeled set per class for training, hold-out validation, and the rest for test. As the nodes that are neither in training, validation, nor test set still influence the training through the parameter K (order of polynomials & receptive field), this split of the dataset can be seen as semi-supervised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52400ec1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52400ec1",
        "outputId": "55985858-6000-4dc9-fb5b-7413a01fb846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140 500 1000\n"
          ]
        }
      ],
      "source": [
        "def canonical_cora_split(y, train_per_class, val_size, test_size, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(y)\n",
        "    classes = np.unique(y)\n",
        "\n",
        "    # 20 labeled nodes per class\n",
        "    train_idx = []\n",
        "    for c in classes:\n",
        "        idx_c = np.where(y == c)[0]\n",
        "        rng.shuffle(idx_c)\n",
        "        take = min(train_per_class, len(idx_c))\n",
        "        train_idx.append(idx_c[:take])\n",
        "    train_idx = np.concatenate(train_idx)\n",
        "\n",
        "    # From the remaining, sample 500 val and 1000 test\n",
        "    remaining = np.setdiff1d(np.arange(n), train_idx, assume_unique=False)\n",
        "    rng.shuffle(remaining)\n",
        "\n",
        "    val_take = min(val_size, len(remaining))\n",
        "    val_idx = remaining[:val_take]\n",
        "\n",
        "    remaining2 = remaining[val_take:]\n",
        "    test_take = min(test_size, len(remaining2))\n",
        "    test_idx = remaining2[:test_take]\n",
        "\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "### Cora has n=2708 and 7 classes\n",
        "train_idx, val_idx, test_idx = canonical_cora_split(y, 20, 500, 1000)\n",
        "print(len(train_idx), len(val_idx), len(test_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "705337e3",
      "metadata": {
        "id": "705337e3"
      },
      "source": [
        "## 6. Train single-layer ChebNet with manual gradients\n",
        "For the forward pass, compute $Z=\\sum_k T_k(\\tilde L)X\\theta_k$. Then compute the gradients on $Z$\n",
        "\n",
        "Forward: Z = sum_k T_k X Theta_k; Softmax; Cross-entropy on labeled nodes only. Backprop only to Theta_k. Precompute T_k X.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a94e3cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a94e3cb",
        "outputId": "b0597903-5f88-4dc8-99d3-37402286f0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train 1.9536/0.114 | val 1.8454/0.468\n",
            "Epoch 010 | train 0.0899/1.000 | val 1.2710/0.640\n",
            "Epoch 020 | train 0.0088/1.000 | val 1.1449/0.634\n",
            "Epoch 030 | train 0.0031/1.000 | val 1.1238/0.636\n",
            "Test accuracy: 0.626\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def one_hot(y, C):\n",
        "    Y = np.zeros((len(y), C), dtype=float)\n",
        "    Y[np.arange(len(y)), y] = 1.0\n",
        "    return Y\n",
        "\n",
        "def softmax(logits):\n",
        "    logits = logits - logits.max(axis=1, keepdims=True)\n",
        "    e = np.exp(logits)\n",
        "    return e / (e.sum(axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "def accuracy(logits, y_true):\n",
        "    return float((logits.argmax(axis=1) == y_true).mean())\n",
        "\n",
        "def compute_grads(TX, Thetas, idx, y_true):\n",
        "    logits = chebnet_forward_pass(TX, Thetas)\n",
        "    P = softmax(logits)\n",
        "    Y = one_hot(y_true, P.shape[1])\n",
        "    G = np.zeros_like(P)\n",
        "    G[idx] = (P[idx] - Y)\n",
        "    grads = [TX[k].T @ G for k in range(len(Thetas))]\n",
        "    loss = -np.mean(np.log(P[idx, y_true] + 1e-12))\n",
        "    return grads, loss, logits\n",
        "\n",
        "def step_adam(params, grads, m, v, t, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=5e-4):\n",
        "    t += 1\n",
        "    for i in range(len(params)):\n",
        "        g = grads[i] + weight_decay*params[i]\n",
        "        m[i] = beta1*m[i] + (1-beta1)*g\n",
        "        v[i] = beta2*v[i] + (1-beta2)*(g*g)\n",
        "        mhat = m[i] / (1-beta1**t)\n",
        "        vhat = v[i] / (1-beta2**t)\n",
        "        params[i] -= lr * mhat / (np.sqrt(vhat) + eps)\n",
        "    return params, m, v, t\n",
        "\n",
        "### Hyperparameters\n",
        "K = 2\n",
        "epochs = 200\n",
        "patience = 20\n",
        "\n",
        "TX = chebyshev_polynomials(L_tilde, X, K)\n",
        "\n",
        "Fin = X.shape[1]; Fout = int(y.max()+1)\n",
        "thetas = [np.random.randn(Fin, Fout)*0.01 for _ in range(K+1)]\n",
        "momentum = [np.zeros_like(t) for t in thetas]\n",
        "velocity = [np.zeros_like(t) for t in thetas]\n",
        "\n",
        "### Initialization\n",
        "t_adam = 0\n",
        "best_val = -1e9\n",
        "best_params = None\n",
        "patience_left = patience\n",
        "\n",
        "### Training loop\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "    grads, train_loss, train_logits = compute_grads(TX, thetas, train_idx, y[train_idx])\n",
        "    thetas, momentum, velocity, t_adam = step_adam(thetas, grads, momentum, velocity, t_adam)\n",
        "\n",
        "    train_acc = accuracy(train_logits[train_idx], y[train_idx])\n",
        "\n",
        "    val_logits = chebnet_forward_pass(TX, thetas)\n",
        "    P_val = softmax(val_logits)\n",
        "    val_loss = -np.mean(np.log(P_val[val_idx, y[val_idx]] + 1e-12))\n",
        "    val_acc = accuracy(val_logits[val_idx], y[val_idx])\n",
        "\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | train {train_loss:.4f}/{train_acc:.3f} | val {val_loss:.4f}/{val_acc:.3f}\")\n",
        "\n",
        "    if val_acc > best_val:\n",
        "        best_val, best_params, patience_left = val_acc, [p.copy() for p in thetas], patience\n",
        "    else:\n",
        "        patience_left -= 1\n",
        "        if patience_left == 0:\n",
        "            break\n",
        "\n",
        "if best_params is not None:\n",
        "    thetas = best_params\n",
        "\n",
        "test_logits = chebnet_forward_pass(TX, thetas)\n",
        "test_acc = accuracy(test_logits[test_idx], y[test_idx])\n",
        "print(\"Test accuracy:\", round(test_acc, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77dd565",
      "metadata": {
        "id": "f77dd565"
      },
      "source": [
        "## 7. The influence of K\n",
        "In addition to reducing the approximation error by allowing bigger polynomials, the parameter $K$ determines from how many hops (jumps from one node to another) away information is aggregated (receptive field). It is analogous to the number of message passing steps in spatial GNNs such as GraphSAGE.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5361f935",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5361f935",
        "outputId": "79390998-c267-4541-e3b2-ffadeedb556d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K=0 -> val=0.476, test=0.480\n",
            "K=1 -> val=0.490, test=0.498\n",
            "K=2 -> val=0.530, test=0.527\n",
            "K=3 -> val=0.570, test=0.572\n",
            "K=4 -> val=0.610, test=0.600\n",
            "K=5 -> val=0.644, test=0.643\n",
            "K=6 -> val=0.692, test=0.679\n",
            "K=7 -> val=0.702, test=0.700\n",
            "K=8 -> val=0.706, test=0.709\n",
            "K=9 -> val=0.722, test=0.734\n",
            "K=10 -> val=0.716, test=0.735\n",
            "K=11 -> val=0.706, test=0.721\n",
            "K=12 -> val=0.708, test=0.719\n",
            "K=13 -> val=0.694, test=0.704\n",
            "K=14 -> val=0.724, test=0.715\n",
            "K=15 -> val=0.714, test=0.708\n",
            "K=16 -> val=0.718, test=0.713\n",
            "K=17 -> val=0.710, test=0.705\n",
            "K=18 -> val=0.702, test=0.694\n",
            "K=19 -> val=0.702, test=0.690\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_chebnet_K(K, lr=0.02, epochs=150):\n",
        "    Lt = scaled_laplacian(laplacian(A))\n",
        "    TX = chebyshev_polynomials(Lt, X, K)\n",
        "    Fin = X.shape[1]\n",
        "    Fout = int(y.max()+1)\n",
        "    Thetas = [np.random.randn(Fin, Fout) * 0.01 for _ in range(K+1)]\n",
        "    momentum = [np.zeros_like(t) for t in Thetas]\n",
        "    velocity = [np.zeros_like(t) for t in Thetas]\n",
        "    t_adam = 0; best_val = -1e9; best = None; patience_left = 15\n",
        "    for epoch in range(1, epochs+1):\n",
        "        grads, train_loss, train_logits = compute_grads(TX, Thetas, train_idx, y[train_idx])\n",
        "        Thetas, momentum, velocity, t_adam = step_adam(Thetas, grads, momentum, velocity, t_adam, lr=lr)\n",
        "\n",
        "        train_acc = accuracy(train_logits[train_idx], y[train_idx])\n",
        "\n",
        "        val_logits = chebnet_forward_pass(TX, Thetas)\n",
        "        P_val = softmax(val_logits)\n",
        "        val_loss = -np.mean(np.log(P_val[val_idx, y[val_idx]] + 1e-12))\n",
        "        val_acc = accuracy(val_logits[val_idx], y[val_idx])\n",
        "\n",
        "\n",
        "        if val_acc > best_val:\n",
        "            best_val, best, patience_left = val_acc, [t.copy() for t in Thetas], 15\n",
        "        else:\n",
        "            patience_left -= 1\n",
        "            if patience_left == 0:\n",
        "                break\n",
        "    if best is not None:\n",
        "        Thetas = best\n",
        "    test_acc = accuracy(chebnet_forward_pass(TX, Thetas)[test_idx], y[test_idx])\n",
        "    return best_val, test_acc\n",
        "\n",
        "for K in range(20):\n",
        "    val_acc, test_acc = train_chebnet_K(K)\n",
        "    print(f\"K={K} -> val={val_acc:.3f}, test={test_acc:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}